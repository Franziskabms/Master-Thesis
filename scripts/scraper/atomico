import json
import time
import hashlib
from datetime import datetime
from pathlib import Path
from urllib.parse import urlparse
import requests
from bs4 import BeautifulSoup
import trafilatura

# -----------------------
# Configuration
# -----------------------
USER_AGENT = "MasterThesisResearchOx/1.0 (contact: franziska.b.m.schaefer@gmail.com)"
HEADERS = {"User-Agent": USER_AGENT}
OUT_DIR = Path("atomico_corpus")
CACHE_DIR = OUT_DIR / "cache_html"
OUT_JSONL = OUT_DIR / "documents.jsonl"

OUT_DIR.mkdir(parents=True, exist_ok=True)
CACHE_DIR.mkdir(parents=True, exist_ok=True)

REQUEST_SLEEP_SEC = 2.0
TIMEOUT = 30

# Wayback CDX API
CDX_QUERIES = [
    "http://web.archive.org/cdx/search/cdx?url=atomico.com/insights/*&output=json&collapse=urlkey&fl=original,timestamp&filter=statuscode:200",
    "http://web.archive.org/cdx/search/cdx?url=www.atomico.com/insights/*&output=json&collapse=urlkey&fl=original,timestamp&filter=statuscode:200",
]

AI_KEYWORDS = [
    "ai", "artificial intelligence", "llm", "agent", "machine learning",
    "foundation model", "generative", "gpt", "deep learning", "automation",
    "model", "intelligence", "software",
]

# -----------------------
# Helpers
# -----------------------
def url_to_cache_path(url: str) -> Path:
    h = hashlib.sha256(url.encode("utf-8")).hexdigest()[:24]
    return CACHE_DIR / f"{h}.html"

def fetch_url(url: str, retries: int = 3) -> str | None:
    cache_path = url_to_cache_path(url)
    if cache_path.exists():
        return cache_path.read_text(encoding="utf-8", errors="ignore")

    for attempt in range(retries):
        try:
            r = requests.get(url, headers=HEADERS, timeout=TIMEOUT)
            if r.status_code == 404:
                return None
            if r.status_code != 200:
                time.sleep(REQUEST_SLEEP_SEC * (attempt + 1))
                continue
            html = r.text
            cache_path.write_text(html, encoding="utf-8")
            time.sleep(REQUEST_SLEEP_SEC)
            return html
        except requests.RequestException as e:
            print(f"  [Error] {url}: {e} (Try {attempt+1}/{retries})")
            time.sleep(REQUEST_SLEEP_SEC * (attempt + 1))
    return None

# -----------------------
# URL-Collection: Wayback CDX API
# -----------------------
def collect_urls_from_cdx() -> dict[str, str]:
    """Returns {clean_url: wayback_url} ."""
    url_map = {}

    for cdx_url in CDX_QUERIES:
        print(f"  Query CDX API: {cdx_url[:60]}...")
        try:
            r = requests.get(cdx_url, headers=HEADERS, timeout=60)
            if r.status_code != 200:
                print(f"  → Error: Status {r.status_code}")
                continue

            data = json.loads(r.text)
            for row in data[1:]:
                original_url = row[0]
                timestamp = row[1] if len(row) > 1 else "20230101000000"

                parsed = urlparse(original_url)
                path = parsed.path.rstrip("/")

                if not original_url.startswith("http"):
                    continue

                if not parsed.path.startswith("/insights/"):
                    continue

                if len(path.split("/")) < 3:
                    continue

                if any(path.endswith(ext) for ext in [".jpg", ".jpeg", ".png", ".gif", ".svg", ".pdf", ".mp4", ".mp3"]):
                    continue

                clean_url = f"https://atomico.com{path}"
                wayback_url = f"https://web.archive.org/web/{timestamp}/https://atomico.com{path}"

                # Only enter if not already present (prefer latest snapshot)
                if clean_url not in url_map or timestamp > url_map[clean_url][0]:
                    url_map[clean_url] = (timestamp, wayback_url)

        except Exception as e:
            print(f"  → CDX Error: {e}")

    print(f"  → {len(url_map)} found clearly Insights-URLs")
    # Gives {clean_url: wayback_url} back
    return {k: v[1] for k, v in url_map.items()}

# -----------------------
# Text-Extraction
# -----------------------
def extract_doc(url: str, html: str) -> dict:
    traf_result = trafilatura.bare_extraction(
        html,
        include_comments=False,
        include_tables=False,
        with_metadata=True,
    )

    if traf_result:
        text = traf_result.text or ""
        title = traf_result.title or ""
        date = traf_result.date or ""
        author = traf_result.author or ""
        tags = traf_result.tags or ""
    else:
        text = trafilatura.extract(html, include_comments=False) or ""
        soup = BeautifulSoup(html, "html.parser")
        title = soup.title.get_text(strip=True) if soup.title else ""
        date, author, tags = "", "", ""

    if not date:
        soup = BeautifulSoup(html, "html.parser")
        for prop in ["article:published_time", "published_time", "og:updated_time"]:
            tag = soup.find("meta", attrs={"property": prop}) or soup.find("meta", attrs={"name": prop})
            if tag and tag.get("content"):
                date = tag["content"].strip()
                break

    # Subheader / Preview Title
    if traf_result and hasattr(traf_result, 'description') and traf_result.description:
        subtitle = traf_result.description or ""
    else:
        soup = BeautifulSoup(html, "html.parser")
        subtitle = ""
        for prop in ["og:description", "description", "twitter:description"]:
            tag = soup.find("meta", attrs={"property": prop}) or soup.find("meta", attrs={"name": prop})
            if tag and tag.get("content"):
                subtitle = tag["content"].strip()
                break

    return {
        "fund": "Atomico",
        "url": url,
        "title": title.strip(),
        "subtitle": subtitle,
        "published_time": date,
        "author": author,
        "tags": tags,
        "retrieved_at": datetime.utcnow().isoformat() + "Z",
        "text": text.strip(),
        "char_count": len(text.strip()),
    }

def save_jsonl(record: dict):
    with OUT_JSONL.open("a", encoding="utf-8") as f:
        f.write(json.dumps(record, ensure_ascii=False) + "\n")

def load_saved_urls() -> set[str]:
    saved = set()
    if not OUT_JSONL.exists():
        return saved
    with OUT_JSONL.open(encoding="utf-8") as f:
        for line in f:
            try:
                saved.add(json.loads(line)["url"])
            except (json.JSONDecodeError, KeyError):
                continue
    return saved

# -----------------------
# Main pipeline
# -----------------------
def build_url_map() -> dict[str, str]:
    """Returns {clean_url: wayback_url} ."""
    print("=== Collect URLs via the Wayback CDX API ===")
    url_map = collect_urls_from_cdx()
    print(f"→ {len(url_map)} all URLs")
    return url_map

def keyword_match(doc: dict, keywords: list[str]) -> bool:
    hay = (doc["title"] + " " + doc["text"]).lower()
    return any(k.lower() in hay for k in keywords)

def run_scrape(max_docs: int = 500, apply_keyword_filter: bool = True):
    url_map = build_url_map()
    urls = sorted(url_map.keys())
    print(f"\n=== start Scrape: {len(urls)} URLs (via Wayback) ===\n")

    saved_urls = load_saved_urls()
    print(f"  saved already: {len(saved_urls)} Documents\n")

    collected = 0
    skipped_short = 0
    skipped_keyword = 0

    for i, clean_url in enumerate(urls):
        if collected >= max_docs:
            break

        if clean_url in saved_urls:
            continue

        wayback_url = url_map[clean_url]
        print(f"[{i+1}/{len(urls)}] {clean_url}")

        # Access via Wayback Machine
        html = fetch_url(wayback_url)
        if not html:
            print("  → unavailable, skipped")
            continue

        # URL in document is original (not  Wayback URL)
        doc = extract_doc(clean_url, html)

        if doc["char_count"] < 500:
            skipped_short += 1
            continue

        if apply_keyword_filter and not keyword_match(doc, AI_KEYWORDS):
            skipped_keyword += 1
            continue

        save_jsonl(doc)
        saved_urls.add(clean_url)
        collected += 1
        print(f"  ✓ saved | {doc['char_count']} Sign | {doc['published_time']}")

    print(f"\n=== Fertig ===")
    print(f"  Saved:         {collected}")
    print(f"  Skipped (short): {skipped_short}")
    print(f"  No Keyword-Match:  {skipped_keyword}")

if __name__ == "__main__":

    # Test Run:
    # run_scrape(max_docs=10, apply_keyword_filter=False)

    # Full Run:
    run_scrape(max_docs=500, apply_keyword_filter=True)
