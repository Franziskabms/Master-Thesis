import json
import time
import hashlib
from datetime import datetime
from pathlib import Path
from urllib.parse import urlparse
import requests
from bs4 import BeautifulSoup
import trafilatura

# -----------------------
# Configuration
# -----------------------
USER_AGENT = "MasterThesisResearchOx/1.0 (contact: franziska.b.m.schaefer@gmail.com)"
HEADERS = {"User-Agent": USER_AGENT}
OUT_DIR = Path("speedinvest_corpus")
CACHE_DIR = OUT_DIR / "cache_html"
OUT_JSONL = OUT_DIR / "documents.jsonl"

OUT_DIR.mkdir(parents=True, exist_ok=True)
CACHE_DIR.mkdir(parents=True, exist_ok=True)

REQUEST_SLEEP_SEC = 2.0
TIMEOUT = 30

# Wayback CDX API
CDX_QUERIES = [
    "http://web.archive.org/cdx/search/cdx?url=speedinvest.com/knowledge/*&output=json&collapse=urlkey&fl=original&filter=statuscode:200",
    "http://web.archive.org/cdx/search/cdx?url=www.speedinvest.com/knowledge/*&output=json&collapse=urlkey&fl=original&filter=statuscode:200",
]

AI_KEYWORDS = [
    "ai", "artificial intelligence", "llm", "agent", "machine learning",
    "foundation model", "generative", "gpt", "deep learning", "automation",
    "model", "intelligence", "software",
]

# -----------------------
# Helpers
# -----------------------
def url_to_cache_path(url: str) -> Path:
    h = hashlib.sha256(url.encode("utf-8")).hexdigest()[:24]
    return CACHE_DIR / f"{h}.html"

def fetch_url(url: str, retries: int = 3) -> str | None:
    cache_path = url_to_cache_path(url)
    if cache_path.exists():
        return cache_path.read_text(encoding="utf-8", errors="ignore")

    for attempt in range(retries):
        try:
            r = requests.get(url, headers=HEADERS, timeout=TIMEOUT)
            if r.status_code == 404:
                return None
            if r.status_code != 200:
                time.sleep(REQUEST_SLEEP_SEC * (attempt + 1))
                continue
            html = r.text
            cache_path.write_text(html, encoding="utf-8")
            time.sleep(REQUEST_SLEEP_SEC)
            return html
        except requests.RequestException as e:
            print(f"  [Error] {url}: {e} (Try {attempt+1}/{retries})")
            time.sleep(REQUEST_SLEEP_SEC * (attempt + 1))
    return None

# -----------------------
# URL-Collection: Wayback CDX API
# -----------------------
def collect_urls_from_cdx() -> set[str]:
    urls = set()

    for cdx_url in CDX_QUERIES:
        print(f"  Frage CDX API ab: {cdx_url[:60]}...")
        try:
            r = requests.get(cdx_url, headers=HEADERS, timeout=60)
            if r.status_code != 200:
                print(f"  → Fehler: Status {r.status_code}")
                continue

            data = json.loads(r.text)
            if len(data) <= 1:
                print(f"  → No URLs found")
                continue

            for row in data[1:]:
                url = row[0]
                parsed = urlparse(url)
                path = parsed.path.rstrip("/")

                if not url.startswith("http"):
                    continue

                if not parsed.path.startswith("/knowledge/"):
                    continue

                if len(path.split("/")) < 3:
                    continue

                if any(path.endswith(ext) for ext in [".jpg", ".jpeg", ".png", ".gif", ".svg", ".pdf", ".mp4", ".mp3"]):
                    continue

                # Remove UTM and other parameters
                clean_url = f"https://www.speedinvest.com{path}"
                urls.add(clean_url)

        except Exception as e:
            print(f"  → CDX Error: {e}")

    print(f"  → {len(urls)} Unique URLs found")
    return urls

# -----------------------
# Text-Extraction
# -----------------------
def extract_doc(url: str, html: str) -> dict:
    traf_result = trafilatura.bare_extraction(
        html,
        include_comments=False,
        include_tables=False,
        with_metadata=True,
    )

    if traf_result:
        text = traf_result.text or ""
        title = traf_result.title or ""
        date = traf_result.date or ""
        author = traf_result.author or ""
        tags = traf_result.tags or ""
    else:
        text = trafilatura.extract(html, include_comments=False) or ""
        soup = BeautifulSoup(html, "html.parser")
        title = soup.title.get_text(strip=True) if soup.title else ""
        date, author, tags = "", "", ""

    if not date:
        soup = BeautifulSoup(html, "html.parser")
        for prop in ["article:published_time", "published_time", "og:updated_time"]:
            tag = soup.find("meta", attrs={"property": prop}) or soup.find("meta", attrs={"name": prop})
            if tag and tag.get("content"):
                date = tag["content"].strip()
                break

    # Subheader / Preview Title
    if traf_result and hasattr(traf_result, 'description') and traf_result.description:
        subtitle = traf_result.description or ""
    else:
        soup = BeautifulSoup(html, "html.parser")
        subtitle = ""
        for prop in ["og:description", "description", "twitter:description"]:
            tag = soup.find("meta", attrs={"property": prop}) or soup.find("meta", attrs={"name": prop})
            if tag and tag.get("content"):
                subtitle = tag["content"].strip()
                break

    return {
        "fund": "Speedinvest",
        "url": url,
        "title": title.strip(),
        "subtitle": subtitle,
        "published_time": date,
        "author": author,
        "tags": tags,
        "retrieved_at": datetime.utcnow().isoformat() + "Z",
        "text": text.strip(),
        "char_count": len(text.strip()),
    }

def save_jsonl(record: dict):
    with OUT_JSONL.open("a", encoding="utf-8") as f:
        f.write(json.dumps(record, ensure_ascii=False) + "\n")

def load_saved_urls() -> set[str]:
    saved = set()
    if not OUT_JSONL.exists():
        return saved
    with OUT_JSONL.open(encoding="utf-8") as f:
        for line in f:
            try:
                saved.add(json.loads(line)["url"])
            except (json.JSONDecodeError, KeyError):
                continue
    return saved

# -----------------------
# Main pipeline
# -----------------------
def build_url_list() -> list[str]:
    print("=== Collect URLs via Wayback CDX API ===")
    all_urls = collect_urls_from_cdx()
    print(f"→ {len(all_urls)} all URLs")
    return sorted(all_urls)

def keyword_match(doc: dict, keywords: list[str]) -> bool:
    hay = (doc["title"] + " " + doc["text"]).lower()
    return any(k.lower() in hay for k in keywords)

def run_scrape(max_docs: int = 500, apply_keyword_filter: bool = True):
    urls = build_url_list()
    print(f"\n=== start Scrape: {len(urls)} URLs ===\n")
    saved_urls = load_saved_urls()
    print(f" already saved: {len(saved_urls)} Documents\n")

    collected = 0
    skipped_short = 0
    skipped_keyword = 0

    for i, url in enumerate(urls):
        if collected >= max_docs:
            break

        if url in saved_urls:
            continue

        print(f"[{i+1}/{len(urls)}] {url}")
        html = fetch_url(url)
        if not html:
            print("  → unavailable, skipped")
            continue

        doc = extract_doc(url, html)

        if doc["char_count"] < 500:
            skipped_short += 1
            continue

        if apply_keyword_filter and not keyword_match(doc, AI_KEYWORDS):
            skipped_keyword += 1
            continue

        save_jsonl(doc)
        saved_urls.add(url)
        collected += 1
        print(f"  ✓ saved | {doc['char_count']} Sign | {doc['published_time']}")

    print(f"\n=== Done ===")
    print(f"  Saved:         {collected}")
    print(f"  Skipped (kurz): {skipped_short}")
    print(f"  No Keyword-Match:  {skipped_keyword}")

if __name__ == "__main__":

    # Test Run:
    # run_scrape(max_docs=10, apply_keyword_filter=False)

    # Full Run:
     run_scrape(max_docs=500, apply_keyword_filter=True)
